[o3-mini]
repo_id = o3-mini
context_length = 200000
min_GPU_RAM = 0
provider = OPENAI

[GPT-4o]
repo_id = chatgpt-4o-latest
context_length = 128000
min_GPU_RAM = 0
provider = OPENAI

[GPT-4o-mini]
repo_id = gpt-4o-mini
context_length = 128000
min_GPU_RAM = 0
provider = OPENAI

[CLAUDE-3.7-SONNET]
repo_id = claude-3-7-sonnet-latest
context_length = 200000
min_GPU_RAM = 0
provider = ANTHROPIC

[CLAUDE-3.5-HAIKU]
repo_id = claude-3-5-haiku-latest
context_length = 200000
min_GPU_RAM = 0
provider = ANTHROPIC

[DEEPSEEK-V3]
repo_id = deepseek-chat
tokenizer = deepseek-ai/DeepSeek-V3
context_length = 64000
min_GPU_RAM = 0
provider = DEEPSEEK

[DEEPSEEK-R1]
repo_id = deepseek-reasoner
tokenizer = deepseek-ai/DeepSeek-R1
context_length = 64000
reason = True
min_GPU_RAM = 0
provider = DEEPSEEK

[LLAMA-4-SCOUT]
repo_id = meta-llama/Llama-4-Scout-17B-16E-Instruct
context_length = 20000000
min_GPU_RAM = 32

[LLAMA-3.3-70B-GGUF]
repo_id = bartowski/Llama-3.3-70B-Instruct-GGUF
file_name = Llama-3.3-70B-Instruct-Q4_K_M.gguf
tokenizer = meta-llama/Llama-3.3-70B-Instruct
context_length = 32000
min_GPU_RAM = 48

[LLAMA-3.3-70B]
repo_id = meta-llama/Llama-3.3-70B-Instruct
context_length = 128000
min_GPU_RAM = 160

[LLAMA-3.1-8B]
repo_id = meta-llama/Meta-Llama-3.1-8B-Instruct
context_length = 128000
min_GPU_RAM = 18

[LLAMA-3.2-3B]
repo_id = meta-llama/Llama-3.2-3B-Instruct
context_length = 128000
min_GPU_RAM = 8

[LLAMA-3.2-1B]
repo_id = meta-llama/Llama-3.2-1B-Instruct
context_length = 128000
min_GPU_RAM = 3

[GEMMA-3-1B]
repo_id = google/gemma-3-1b-it
context_length = 128000
min_GPU_RAM = 4

[GEMMA-3-4B]
repo_id = google/gemma-3-4b-it
context_length = 128000
min_GPU_RAM = 12

[GEMMA-3-12B]
repo_id = google/gemma-3-12b-it
context_length = 128000
min_GPU_RAM = 30

[GEMMA-3-27B]
repo_id = google/gemma-3-27b-it
context_length = 128000
min_GPU_RAM = 80

[GEMMA-3-12B-GGUF]
repo_id = lmstudio-community/gemma-3-12b-it-GGUF
file_name = gemma-3-12b-it-Q4_K_M.gguf
tokenizer = google/gemma-3-12b-it
context_length = 128000
min_GPU_RAM = 10

[GEMMA-3-27B-GGUF]
repo_id = lmstudio-community/gemma-3-27b-it-GGUF
file_name = gemma-3-27b-it-Q4_K_M.gguf
tokenizer = google/gemma-3-27b-it
context_length = 128000
min_GPU_RAM = 20

[QWEN-QwQ-32B]
repo_id = Qwen/QwQ-32B
context_length = 152000
min_GPU_RAM = 80
reason = True

[QWEN-QwQ-32B-GGUF]
repo_id = Qwen/QwQ-32B-GGUF
file_name = qwq-32b-q4_k_m.gguf
tokenizer = Qwen/QwQ-32B
context_length = 4096
min_GPU_RAM = 24
reason = True

[DEEPSEEK-R1-DISTILL-QWEN-32B]
repo_id = deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
context_length = 128000
min_GPU_RAM = 100
reason = True

[DEEPSEEK-R1-DISTILL-QWEN-32B-GGUF]
repo_id = unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF
file_name = DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf
tokenizer = deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
context_length = 128000
min_GPU_RAM = 36
reason = True

[DEEPSEEK-R1-DISTILL-QWEN-7B]
repo_id = deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
context_length = 128000
min_GPU_RAM = 16
reason = True

[DEEPSEEK-R1-DISTILL-QWEN-7B-GGUF]
repo_id = unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF
file_name = DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf
tokenizer = deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
context_length = 16000
min_GPU_RAM = 6
reason = True

[DEEPSEEK-R1-DISTILL-LLAMA-8B]
repo_id = deepseek-ai/DeepSeek-R1-Distill-Llama-8B
context_length = 128000
min_GPU_RAM = 18
reason = True

[DEEPSEEK-R1-DISTILL-LLAMA-8B-GGUF]
repo_id = unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF
file_name = DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf
tokenizer = meta-llama/Meta-Llama-3.1-8B-Instruct
context_length = 16000
min_GPU_RAM = 6
reason = True